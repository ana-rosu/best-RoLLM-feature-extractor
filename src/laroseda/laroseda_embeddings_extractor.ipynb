{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c8ba4d8-59df-4360-8bc2-79b8ce82d1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "splits = {}\n",
    "for split_name in ['train', 'validation', 'test']:\n",
    "    csv_path = f\"{split_name}.csv\"\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        splits[split_name] = {\n",
    "            'index': df['index'].tolist(),\n",
    "            'title': df['title'].tolist(),\n",
    "            'content': df['content'].tolist(),\n",
    "            'starRating': df['starRating'].tolist(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690575a8-9fda-41b4-adb7-fdd3141e7455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from enum import Enum\n",
    "\n",
    "MODEL_NORMALIZATION = {\n",
    "    \"faur-ai/LLMic\": True\n",
    "}\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "class EmbeddingExtractor:\n",
    "    def __init__(self, model, tokenizer, model_name, device=None, pooling=\"classical-avg\"):\n",
    "        self.model = model.to(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pooling = pooling\n",
    "        self.model_name = model_name\n",
    "        self.model_needs_normalization = MODEL_NORMALIZATION.get(self.model_name, False)\n",
    "        self.max_length = getattr(self.model.config, \"max_position_embeddings\")\n",
    "        \n",
    "    def _build_prompt(self, text, strategy):\n",
    "        if strategy == \"echo\":\n",
    "            prompt = f\"Rescrie recenzia: {text}. Recenzia rescrisă: {text}.\"\n",
    "        elif strategy == \"summary\":\n",
    "            prompt = f\"Rezumă recenzia: {text}. Răspunde doar cu un cuvânt:\"\n",
    "        else:\n",
    "            prompt = f\"Scrie recenzia: {text}.\"\n",
    "\n",
    "        if self.model_needs_normalization:\n",
    "          return remove_diacritics(prompt).lower()\n",
    "        return prompt\n",
    "\n",
    "    def _apply_pooling(self, hidden_states, inputs, text, strategy, pooling_method):\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        full_text = self.tokenizer.decode(input_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if self.model_needs_normalization:\n",
    "            full_text = remove_diacritics(full_text).lower()\n",
    "\n",
    "        # print(f\"\\n[DEBUG] Full Decoded Prompt:\\n{full_text}\\n\")\n",
    "\n",
    "        if strategy == \"echo\":\n",
    "            instruction = \"recenzia rescrisa:\" if self.model_needs_normalization else \"Recenzia rescrisă:\"\n",
    "            second_occurrence_idx = full_text.rfind(text)\n",
    "            selected_text = full_text[second_occurrence_idx:]\n",
    "            # print(f\"[DEBUG] Selected (echo second occurrence):\\n{selected_text}\\n\")\n",
    "            second_tokens = self.tokenizer(selected_text, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = second_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :]\n",
    "\n",
    "        elif strategy == \"summary\":\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            completion = generated_text[len(full_text):].strip()\n",
    "            # print(f\"[DEBUG] Generated Text (summary):\\n{completion}\\n\")\n",
    "            first_word = completion.split()[0] if completion else \"\"\n",
    "            # print(f\"[DEBUG] Generated Text First Word (summary):\\n{first_word}\\n\")\n",
    "            if self.model_name == \"faur-ai/LLMic\":\n",
    "                first_word = next((w for w in completion.split() if w.isalnum() and len(w) >= 3), \"\")\n",
    "                # print(f\"[DEBUG] Generated Text First Word LLMIC (summary):\\n{first_word}\\n\")\n",
    "            if not completion:\n",
    "                print(\"[WARNING] Model did not generate anything after summary instruction.\")\n",
    "\n",
    "            summary_tokens = self.tokenizer(first_word, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = summary_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :] if length > 0 else hidden_states[:, -1:, :]\n",
    "\n",
    "        else:\n",
    "            instruction = \"scrie recenzia:\" if self.model_needs_normalization else \"Scrie recenzia:\"\n",
    "            idx = full_text.find(instruction)\n",
    "            if idx == -1:\n",
    "                raise ValueError(f\"Failed to find classical instruction '{instruction}' in the prompt text.\")\n",
    "            after_instruction = full_text[idx + len(instruction):].strip()\n",
    "            # print(f\"[DEBUG] Selected (classical real input):\\n{after_instruction}\\n\")\n",
    "            text_tokens = self.tokenizer(after_instruction, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = text_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :]\n",
    "\n",
    "        if pooling_method == \"avg\":\n",
    "            return selected.mean(dim=1).squeeze()\n",
    "        elif pooling_method == \"last\":\n",
    "            return selected[:, -1, :].squeeze()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {pooling_method}\")\n",
    "\n",
    "\n",
    "    def extract_single(self, text):\n",
    "        strategy, pooling_method = self.pooling.split(\"-\")\n",
    "        prompt = self._build_prompt(text, strategy)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True,  max_length=self.max_length).to(self.device)\n",
    "        \n",
    "        # print(\"[DEBUG] Decoded back:\", self.tokenizer.decode(inputs[\"input_ids\"][0])) # see how model internally tokenizes data (lowercase etc) llmic fara diactritice si lowercase\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "        text = remove_diacritics(text).lower() if self.model_needs_normalization else text\n",
    "        return self._apply_pooling(hidden_states, inputs, text, strategy, pooling_method)\n",
    "\n",
    "    def extract_batch(self, texts, save_path=None, save_format=\"pt\"):\n",
    "        embeddings = []\n",
    "\n",
    "        for text in tqdm(texts, desc=f\"Extracting ({self.pooling})\"):\n",
    "            emb = self.extract_single(text)\n",
    "            embeddings.append(emb.cpu())\n",
    "\n",
    "        stacked = torch.stack(embeddings)\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if save_format == \"pt\":\n",
    "                torch.save(stacked, save_path)\n",
    "            elif save_format == \"npy\":\n",
    "                import numpy as np\n",
    "                np.save(save_path, stacked.numpy())\n",
    "\n",
    "        return stacked\n",
    "\n",
    "    def extract_in_chunks(extractor, texts, save_path, save_format=\"pt\", batch_size=64):\n",
    "        all_embeddings = []\n",
    "    \n",
    "        total = len(texts)\n",
    "        with tqdm(total=total, desc=f\"Extracting ({extractor.pooling})\") as pbar:\n",
    "            for i in range(0, total, batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                batch_emb = extractor.extract_batch(batch)  \n",
    "                all_embeddings.append(batch_emb)\n",
    "                pbar.update(len(batch))\n",
    "    \n",
    "        stacked = torch.cat(all_embeddings)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf57123-8790-4a00-a105-09bde706c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingExtractionRunner:\n",
    "    def __init__(self, model, tokenizer, model_name, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self, splits, save_root):\n",
    "        strategies = [\"classical-avg\", \"classical-last\", \"echo-avg\", \"summary-avg\"]\n",
    "        fields = [\"title\", \"content\", \"title+content\"]\n",
    "\n",
    "        for strategy in strategies:\n",
    "            for field in fields:\n",
    "                extractor = EmbeddingExtractor(\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    model_name=self.model_name,\n",
    "                    device=self.device,\n",
    "                    pooling=strategy\n",
    "                )\n",
    "\n",
    "                for split_name, split_data in splits.items():\n",
    "                    print(f\"field={field}, split={split_name}\")\n",
    "                    if field == \"title\":\n",
    "                        texts = split_data['title']\n",
    "                    elif field == \"content\":\n",
    "                        texts = split_data['content']\n",
    "                    elif field == \"title+content\":\n",
    "                        texts = [f\"{t} {c}\" for t, c in zip(split_data['title'], split_data['content'])]\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported field type: {field}\")\n",
    "\n",
    "                    base_dir = os.path.join(\n",
    "                        save_root,\n",
    "                        self.model_name,\n",
    "                        strategy,\n",
    "                        field,\n",
    "                        split_name\n",
    "                    )\n",
    "                    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "                    pt_path = os.path.join(base_dir, \"embeddings.pt\")\n",
    "                    npy_path = os.path.join(base_dir, \"embeddings.npy\")\n",
    "\n",
    "                    embeddings = extractor.extract_batch(texts) \n",
    "                    torch.save(embeddings, pt_path)\n",
    "                    np.save(npy_path, embeddings.cpu().numpy())\n",
    "\n",
    "                    del embeddings\n",
    "                    torch.cuda.empty_cache()\n",
    "                    import gc\n",
    "                    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda2f28-ff61-4287-afd6-a95e6a15c46a",
   "metadata": {},
   "source": [
    "RoLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "123ca45d-fe86-496c-92a9-c562fd6c86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100% 4/4 [00:01<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_NAME = \"OpenLLM-Ro/RoLlama3.1-8b-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56f50d18-7a95-4ef2-99cb-56dfc532778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Full Decoded Prompt:\n",
      "Rezumă recenzia: un produs excelent care mi-a depasit asteptarile.  se incarca \"fast\" si incarca \"fast\" la randul lui.  recomand cu caldura.. Răspunde doar cu un cuvânt:\n",
      "\n",
      "[DEBUG] Generated Text First Word (summary):\n",
      "excelent. Răspunde doRecenz\n",
      "\n",
      "[DEBUG] Generated Text First Word (summary):\n",
      "excelent.\n",
      "\n",
      "Embedding shape: torch.Size([4096])\n",
      "Embedding preview: tensor([-3.0418,  0.7847,  5.7454,  0.3592,  1.6239, -2.5949, -0.8607, -1.4430,\n",
      "        -3.5565,  1.5602], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "extractor = EmbeddingExtractor(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"cuda\",\n",
    "    pooling=\"summary-avg\"\n",
    ")\n",
    "\n",
    "text = splits[\"train\"][\"content\"][3]\n",
    "\n",
    "embedding = extractor.extract_single(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding preview: {embedding[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc8b29-652d-4fa4-98c4-8f2b7c116a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = EmbeddingExtractionRunner(model, tokenizer, model_name=\"OpenLLM-Ro/RoLlama3.1-8b-Instruct\")\n",
    "runner.run(splits, save_root=\"outputs_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b7d81c-69cd-4cb1-b2c6-43e2ef82fcf1",
   "metadata": {},
   "source": [
    "MGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "919db978-7851-434b-aa04-d0b66925e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_NAME = \"ai-forever/mGPT-1.3B-romanian\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0611f13d-62f8-4b3c-a36b-b4c171769a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Generated Text (summary):\n",
      "\"fast\".\n",
      "\n",
      "- Nu-mi place\n",
      "\n",
      "[DEBUG] Generated Text First Word (summary):\n",
      "\"fast\".\n",
      "\n",
      "Embedding shape: torch.Size([2048])\n",
      "Embedding preview: tensor([ 0.1807, -1.0960, -0.1395,  0.5721,  0.5812, -0.0932,  0.7749,  1.6887,\n",
      "        -0.1019,  0.6627], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "extractor = EmbeddingExtractor(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"cuda\",\n",
    "    pooling=\"summary-avg\"\n",
    ")\n",
    "\n",
    "text = splits[\"train\"][\"content\"][3]\n",
    "\n",
    "embedding = extractor.extract_single(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding preview: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916f6ad-249b-4f35-bc65-cf2c74b69d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = EmbeddingExtractionRunner(model, tokenizer, model_name=\"ai-forever/mGPT-1.3B-romanian\")\n",
    "runner.run(splits, save_root=\"outputs_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7c87d-7cd0-4e1e-9f3a-4482e2564f1e",
   "metadata": {},
   "source": [
    "Llmic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9198c51-d5db-4e98-80c7-9bd440312626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d921056c560f44f49f34a688d1daac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_NAME = \"faur-ai/LLMic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2028030a-44fa-4b00-b929-88358b95bd44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([2560])\n",
      "Embedding preview: tensor([-7.0801, -4.2104, -2.0157, 13.6697,  9.5080, -7.7106, -6.3963, -5.4862,\n",
      "         3.0037, 33.4825], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "extractor = EmbeddingExtractor(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"cuda\",\n",
    "    pooling=\"summary-avg\"\n",
    ")\n",
    "\n",
    "text = splits[\"train\"][\"content\"][3]\n",
    "\n",
    "embedding = extractor.extract_single(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding preview: {embedding[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc3501-42b8-4c7d-a844-06514d6a0321",
   "metadata": {},
   "source": [
    "It generates the same thing no matter what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037bc948-5b4a-4469-b808-3e8c3c05606f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = EmbeddingExtractionRunner(model, tokenizer, model_name=\"faur-ai/LLMic\")\n",
    "runner.run(splits, save_root=\"outputs_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b97854-6234-4e90-96c4-6d89136a90c5",
   "metadata": {},
   "source": [
    "LLama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d3924-48ff-41ee-81e4-cc7dd8b802ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6391ae08-3643-4df7-9541-90e86fb3598a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b31ea071-6e5d-4984-9b3a-374ed7c3db44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b899dcd7e54171a2631efadb1dd150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51859b70-16bc-4753-a754-998d8c3be675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([4096])\n",
      "Embedding preview: tensor([-3.0598,  1.6368,  4.4468,  0.6540,  2.1140, -3.9921, -0.7091, -0.9007,\n",
      "        -2.7611,  0.9963], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "extractor = EmbeddingExtractor(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name=MODEL_NAME,\n",
    "    device=\"cuda\",\n",
    "    pooling=\"summary-avg\"\n",
    ")\n",
    "\n",
    "text = splits[\"train\"][\"content\"][3]\n",
    "\n",
    "embedding = extractor.extract_single(text)\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding preview: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e8039-7731-4d6f-bbb2-31203d183c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EmbeddingExtractionRunner(model, tokenizer, model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "runner.run(splits, save_root=\"outputs_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
