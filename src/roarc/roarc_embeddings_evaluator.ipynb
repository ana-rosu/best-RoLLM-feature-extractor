{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('OpenLLM-Ro/ro_arc_challenge', trust_remote_code=True)\n",
    "\n",
    "def preprocess_splits(dataset):\n",
    "    train_df = dataset['train'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "    test_df = dataset['test'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "    validation_df = dataset['validation'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "\n",
    "    train_df = train_df[train_df['answer'] != 'E']\n",
    "    train_df = train_df[train_df['option_e'].isnull()]\n",
    "    test_df = test_df[test_df['answer'] != 'E']\n",
    "    test_df = test_df[test_df['option_e'].isnull()]\n",
    "    validation_df = validation_df[validation_df['answer'] != 'E']\n",
    "    validation_df = validation_df[validation_df['option_e'].isnull()]\n",
    "    for df in (train_df, validation_df, test_df):\n",
    "        df.drop(columns=[\"option_e\"], inplace=True)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_df,\n",
    "        \"validation\": validation_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "    return splits\n",
    "\n",
    "splits = preprocess_splits(dataset)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0754fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_ROOT = \"roarc_embeddings\"\n",
    "strategies = [\"classical-avg\", \"classical-last\", \"echo-avg\", \"summary-avg\"]\n",
    "model_names = {\n",
    "    \"llama3\": \"Llama-3.1-8B-Instruct\",\n",
    "    \"rollama\": \"RoLlama3.1-8b-Instruct\",\n",
    "    \"mgpt\": \"mGPT-1.3B-romanian\",\n",
    "    \"llmic\": \"faur-ai/LLMic\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fb7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    if path.endswith(\".pt\"):\n",
    "        return torch.load(path)\n",
    "    elif path.endswith(\".npy\"):\n",
    "        return torch.tensor(np.load(path))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "429fcba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def train_mlp_classifier(X_train, y_train, X_val, y_val, input_dim, device):\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "\n",
    "    class MLP(nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(input_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 4)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)\n",
    "\n",
    "    model = MLP(input_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X_train)\n",
    "        loss = criterion(logits, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds = torch.argmax(model(X_val), dim=1)\n",
    "        acc = accuracy_score(y_val.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        f1 = f1_score(y_val.cpu().numpy(), val_preds.cpu().numpy(), average=\"macro\")\n",
    "\n",
    "    return model, acc, f1\n",
    "\n",
    "def answer_to_index(ans):\n",
    "    return {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}.get(ans.strip().upper(), -1)\n",
    "\n",
    "def run_all_classifiers(root_dir, splits):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "\n",
    "    for model_name in os.listdir(root_dir): \n",
    "            model_path = os.path.join(root_dir, model_name)\n",
    "\n",
    "            for strategy in strategies:\n",
    "                try:\n",
    "                    print(f\"Running MLP for: {model_name} / {strategy}\")\n",
    "\n",
    "                    def load_split(split):\n",
    "                        path = os.path.join(model_path, strategy, split, \"embeddings.npy\")\n",
    "                        return load_embeddings(path)\n",
    "\n",
    "                    X_train = load_split(\"train\")\n",
    "                    y_train = [answer_to_index(a) for a in splits['train']['answer']]\n",
    "\n",
    "                    X_val = load_split(\"validation\")\n",
    "                    y_val = [answer_to_index(a) for a in splits['validation']['answer']]\n",
    "\n",
    "                    X_test = load_split(\"test\")\n",
    "                    y_test = [answer_to_index(a) for a in splits['test']['answer']]\n",
    "\n",
    "                    if not (len(X_train) == len(y_train) and len(X_val) == len(y_val)):\n",
    "                        print(\"Length mismatch, skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    model, acc, f1 = train_mlp_classifier(X_train, y_train, X_val, y_val, X_train.shape[1], device)\n",
    "\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "                        test_preds = torch.argmax(model(X_test), dim=1)\n",
    "                        test_acc = accuracy_score(y_test, test_preds.cpu().numpy())\n",
    "                        test_f1 = f1_score(y_test, test_preds.cpu().numpy(), average=\"macro\")\n",
    "                        test_cm = confusion_matrix(y_test, test_preds.cpu().numpy())\n",
    "                        test_report = classification_report(y_test, test_preds.cpu().numpy(), target_names=[\"A\", \"B\", \"C\", \"D\"])\n",
    "\n",
    "                    results[(model_name, strategy)] = {\n",
    "                        \"val_acc\": acc,\n",
    "                        \"val_f1\": f1,\n",
    "                        \"test_acc\": test_acc,\n",
    "                        \"test_f1\": test_f1,\n",
    "                        \"confusion_matrix\": test_cm.tolist(),\n",
    "                        \"classification_report\": test_report\n",
    "                    }\n",
    "                    # print(f\"Classification report for {model_name} / {strategy}:\\n{test_report}\")\n",
    "                    import seaborn as sns\n",
    "                    import matplotlib.pyplot as plt\n",
    "                    import numpy as np\n",
    "                    from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "                    def plot_confusion_matrix(cm, model_name, strategy, labels=[\"A\", \"B\", \"C\", \"D\"]):\n",
    "                        plt.figure(figsize=(6, 5))\n",
    "                        sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "                        plt.xlabel(\"Predicted\")\n",
    "                        plt.ylabel(\"Actual\")\n",
    "                        plt.title(f\"Confusion Matrix: {model_name} / {strategy}\")\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "                    # plot_confusion_matrix(test_cm, model_name, strategy)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed for {model_name}/{strategy}: {e}\")\n",
    "                    continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd06cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all_classifiers(MODEL_ROOT, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1294a9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Val Acc</th>\n",
       "      <th>Val F1</th>\n",
       "      <th>Test Acc</th>\n",
       "      <th>Test F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3</td>\n",
       "      <td>classical-last</td>\n",
       "      <td>34.80</td>\n",
       "      <td>33.58</td>\n",
       "      <td>36.08</td>\n",
       "      <td>35.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rollama</td>\n",
       "      <td>classical-last</td>\n",
       "      <td>33.78</td>\n",
       "      <td>32.73</td>\n",
       "      <td>34.11</td>\n",
       "      <td>34.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rollama</td>\n",
       "      <td>summary-avg</td>\n",
       "      <td>38.51</td>\n",
       "      <td>37.50</td>\n",
       "      <td>29.64</td>\n",
       "      <td>29.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>rollama</td>\n",
       "      <td>echo-avg</td>\n",
       "      <td>29.39</td>\n",
       "      <td>28.69</td>\n",
       "      <td>28.09</td>\n",
       "      <td>27.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mgpt</td>\n",
       "      <td>classical-last</td>\n",
       "      <td>28.72</td>\n",
       "      <td>28.46</td>\n",
       "      <td>27.66</td>\n",
       "      <td>27.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>llama3</td>\n",
       "      <td>echo-avg</td>\n",
       "      <td>28.38</td>\n",
       "      <td>27.20</td>\n",
       "      <td>26.89</td>\n",
       "      <td>26.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llmic</td>\n",
       "      <td>summary-avg</td>\n",
       "      <td>25.34</td>\n",
       "      <td>11.62</td>\n",
       "      <td>26.80</td>\n",
       "      <td>11.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llmic</td>\n",
       "      <td>echo-avg</td>\n",
       "      <td>26.01</td>\n",
       "      <td>22.29</td>\n",
       "      <td>26.55</td>\n",
       "      <td>22.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>llmic</td>\n",
       "      <td>classical-last</td>\n",
       "      <td>24.32</td>\n",
       "      <td>9.78</td>\n",
       "      <td>26.55</td>\n",
       "      <td>10.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mgpt</td>\n",
       "      <td>classical-avg</td>\n",
       "      <td>27.03</td>\n",
       "      <td>25.99</td>\n",
       "      <td>24.91</td>\n",
       "      <td>23.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>mgpt</td>\n",
       "      <td>summary-avg</td>\n",
       "      <td>29.39</td>\n",
       "      <td>27.80</td>\n",
       "      <td>24.57</td>\n",
       "      <td>24.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llama3</td>\n",
       "      <td>summary-avg</td>\n",
       "      <td>27.70</td>\n",
       "      <td>27.08</td>\n",
       "      <td>24.40</td>\n",
       "      <td>23.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>llmic</td>\n",
       "      <td>classical-avg</td>\n",
       "      <td>23.65</td>\n",
       "      <td>20.21</td>\n",
       "      <td>24.05</td>\n",
       "      <td>20.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>llama3</td>\n",
       "      <td>classical-avg</td>\n",
       "      <td>31.08</td>\n",
       "      <td>31.00</td>\n",
       "      <td>23.88</td>\n",
       "      <td>23.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rollama</td>\n",
       "      <td>classical-avg</td>\n",
       "      <td>29.73</td>\n",
       "      <td>28.61</td>\n",
       "      <td>23.45</td>\n",
       "      <td>23.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>mgpt</td>\n",
       "      <td>echo-avg</td>\n",
       "      <td>26.01</td>\n",
       "      <td>24.35</td>\n",
       "      <td>22.77</td>\n",
       "      <td>21.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model        Strategy  Val Acc  Val F1  Test Acc  Test F1\n",
       "1    llama3  classical-last    34.80   33.58     36.08    35.37\n",
       "13  rollama  classical-last    33.78   32.73     34.11    34.10\n",
       "15  rollama     summary-avg    38.51   37.50     29.64    29.08\n",
       "14  rollama        echo-avg    29.39   28.69     28.09    27.41\n",
       "9      mgpt  classical-last    28.72   28.46     27.66    27.53\n",
       "2    llama3        echo-avg    28.38   27.20     26.89    26.76\n",
       "7     llmic     summary-avg    25.34   11.62     26.80    11.35\n",
       "6     llmic        echo-avg    26.01   22.29     26.55    22.57\n",
       "5     llmic  classical-last    24.32    9.78     26.55    10.49\n",
       "8      mgpt   classical-avg    27.03   25.99     24.91    23.08\n",
       "11     mgpt     summary-avg    29.39   27.80     24.57    24.10\n",
       "3    llama3     summary-avg    27.70   27.08     24.40    23.79\n",
       "4     llmic   classical-avg    23.65   20.21     24.05    20.49\n",
       "0    llama3   classical-avg    31.08   31.00     23.88    23.82\n",
       "12  rollama   classical-avg    29.73   28.61     23.45    23.23\n",
       "10     mgpt        echo-avg    26.01   24.35     22.77    21.39"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "records = []\n",
    "for (model, strategy), scores in results.items():\n",
    "    row = {\n",
    "        \"Model\": model,\n",
    "        \"Strategy\": strategy,\n",
    "        \"Val Acc\": round(scores[\"val_acc\"] * 100, 2),\n",
    "        \"Val F1\": round(scores[\"val_f1\"] * 100, 2),\n",
    "        \"Test Acc\": round(scores[\"test_acc\"] * 100, 2),\n",
    "        \"Test F1\": round(scores[\"test_f1\"] * 100, 2),\n",
    "    }\n",
    "    records.append(row)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df_sorted = df.sort_values(by=[\"Test Acc\", \"Test F1\"], ascending=False)\n",
    "df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
