{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72666f6-0e61-4e46-bde5-b063166616c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train':                                                option_d  \\\n",
       " 0     Au fost formați din rămășițele plantelor și an...   \n",
       " 1     Mai puține poluanții vor fi produși de vehicul...   \n",
       " 2     patru elevi care lucrează împreună pentru a mu...   \n",
       " 3                                  Ele creează energie.   \n",
       " 4     Un organism care depinde de surse similare de ...   \n",
       " ...                                                 ...   \n",
       " 1104  de la greutatea sedimentelor care presează asu...   \n",
       " 1105                      de la Carul Mare la Carul Mic   \n",
       " 1106  hidrogen și oxigen reacționând pentru a produc...   \n",
       " 1107          folosește două mărgele de aceeași mărime.   \n",
       " 1108  reduces procentul de căldură pierdută în atmos...   \n",
       " \n",
       "                                          id  \\\n",
       " 0        ARC-Challenge/train/Mercury_402062   \n",
       " 1        ARC-Challenge/train/MDSA_2010_5_35   \n",
       " 2     ARC-Challenge/train/Mercury_SC_416461   \n",
       " 3       ARC-Challenge/train/Mercury_7185448   \n",
       " 4         ARC-Challenge/train/MCAS_2003_8_8   \n",
       " ...                                     ...   \n",
       " 1104     ARC-Challenge/train/Mercury_187618   \n",
       " 1105    ARC-Challenge/train/Mercury_7111248   \n",
       " 1106   ARC-Challenge/train/MCAS_2007_8_5169   \n",
       " 1107  ARC-Challenge/train/Mercury_SC_400174   \n",
       " 1108    ARC-Challenge/train/Mercury_7097248   \n",
       " \n",
       "                                                option_c answer  \\\n",
       " 0     Sunt folosiți pentru încălzirea caselor și a a...      D   \n",
       " 1     Mai puține poluanții vor fi eliberați de vehic...      C   \n",
       " 2         doi elevi care colectează gunoi de la un grup      B   \n",
       " 3                                  Ele schimbă volumul.      B   \n",
       " 4     Condițiile meteorologice anormale au redus niv...      B   \n",
       " ...                                                 ...    ...   \n",
       " 1104  de la roci aflate sub stres care se mută adânc...      C   \n",
       " 1105                    de la Soare la Proxima Centauri      B   \n",
       " 1106                 sodiu și clor formând sare de masă      B   \n",
       " 1107                înclina rampa la unghiuri diferite.      B   \n",
       " 1108  necesită mai mult curent electric decât ventil...      D   \n",
       " \n",
       "                                                option_a  \\\n",
       " 0                                Au fost cândva fosile.   \n",
       " 1                               Mediul nu va fi poluat.   \n",
       " 2                   un elev care servește apă unui grup   \n",
       " 3                          Ele schimbă starea materiei.   \n",
       " 4       Competiția pentru hrană a crescut între iepuri.   \n",
       " ...                                                 ...   \n",
       " 1104  de la o creștere bruscă a radiației solare car...   \n",
       " 1105                           de la galaxie la galaxie   \n",
       " 1106               rugină formându-se pe un cui de fier   \n",
       " 1107          elibereze mărgelele la înălțimi diferite.   \n",
       " 1108    transformă mai mult curent electric în căldură.   \n",
       " \n",
       "                                                option_b  \\\n",
       " 0               Au fost formați în timpuri preistorice.   \n",
       " 1                          Mediul va deveni mai poluat.   \n",
       " 2                     un elev care ia cereale din dulap   \n",
       " 3                            Ele creează noi substanțe.   \n",
       " 4     Principalul prădător al iepurilor a fost elimi...   \n",
       " ...                                                 ...   \n",
       " 1104  de la tracțiunea gravitațională a Lunii în tim...   \n",
       " 1105                             de la Saturn la Mercur   \n",
       " 1106             cristale de zahăr dizolvându-se în apă   \n",
       " 1107              repete experimentul de mai multe ori.   \n",
       " 1108  se rotește la o viteză mai mică decât ventilat...   \n",
       " \n",
       "                                             instruction  \n",
       " 0     De ce sunt numite combustibili fosili cărbunel...  \n",
       " 1     Multe state cer ca vehiculele să fie examinate...  \n",
       " 2     O clasă modelează diferențele dintre un organi...  \n",
       " 3     Aprinderea unei chibrituri și coacerea unui to...  \n",
       " 4     Populația de iepuri a crescut semnificativ în ...  \n",
       " ...                                                 ...  \n",
       " 1104       Unde își are originea energia unui cutremur?  \n",
       " 1105  O unitate astronomică este distanța medie dint...  \n",
       " 1106  Care dintre următoarele reprezintă un exemplu ...  \n",
       " 1107  Un student a comparat vitezele la care o mărge...  \n",
       " 1108  Un producător de aparate electrocasnice și-a r...  \n",
       " \n",
       " [1108 rows x 7 columns],\n",
       " 'validation':                                               option_d  \\\n",
       " 0                                     afișarea datelor   \n",
       " 1                                     scade în viteză.   \n",
       " 2    Apa rece, apa călduță și apa fierbinte au dizo...   \n",
       " 3      Fluturii și molii au același număr de picioare.   \n",
       " 4    Trebuie pusă într-un container pentru eliminar...   \n",
       " ..                                                 ...   \n",
       " 292                  Compoziția materiei s-a schimbat.   \n",
       " 293             spălarea stropului chimic cu multă apă   \n",
       " 294  Temperatura lichidului a scăzut sau lichidul a...   \n",
       " 295  Transpirația trebuie să adauge vapori de apă î...   \n",
       " 296                      primul sfert și ultimul sfert   \n",
       " \n",
       "                                              id  \\\n",
       " 0       ARC-Challenge/validation/Mercury_192168   \n",
       " 1      ARC-Challenge/validation/Mercury_7085820   \n",
       " 2    ARC-Challenge/validation/TIMSS_2007_4_pg90   \n",
       " 3      ARC-Challenge/validation/Mercury_7071365   \n",
       " 4      ARC-Challenge/validation/Mercury_7042735   \n",
       " ..                                          ...   \n",
       " 292    ARC-Challenge/validation/Mercury_7221865   \n",
       " 293    ARC-Challenge/validation/Mercury_7017990   \n",
       " 294  ARC-Challenge/validation/Mercury_SC_412337   \n",
       " 295   ARC-Challenge/validation/NCEOGA_2013_5_43   \n",
       " 296    ARC-Challenge/validation/Mercury_7171938   \n",
       " \n",
       "                                               option_c answer  \\\n",
       " 0                                 permite vizualizarea      D   \n",
       " 1                                crește în dimensiune.      D   \n",
       " 2    Apa fierbinte a dizolvat cea mai multă cantita...      C   \n",
       " 3    Fluturii și molii sunt mult mai fericiți în ti...      D   \n",
       " 4        Trebuie versată într-un container de deșeuri.      C   \n",
       " ..                                                 ...    ...   \n",
       " 292   Materia a fost conservată când a schimbat forma.      C   \n",
       " 293                             înnodarea părului lung      A   \n",
       " 294  Temperatura lichidului a scăzut sau lichidul a...      D   \n",
       " 295  Precipitațiile trebuie să înceapă să cadă și s...      B   \n",
       " 296                         ultimul sfert și nouă lună      D   \n",
       " \n",
       "                                               option_a  \\\n",
       " 0                    simularea fenomenelor științifice   \n",
       " 1                             câștigă energie termică.   \n",
       " 2    Apa rece a dizolvat cea mai multă cantitate de...   \n",
       " 3    Fluturii sunt mai frumoși și mai amuzanți de p...   \n",
       " 4           Trebuie versată într-un canal de scurgere.   \n",
       " ..                                                 ...   \n",
       " 292                    S-a creat materie nouă în bule.   \n",
       " 293                        degustarea probelor chimice   \n",
       " 294  Temperatura lichidului a crescut sau lichidul ...   \n",
       " 295          Vaporul de apă trebuie să se încălzească.   \n",
       " 296                               plin și primul sfert   \n",
       " \n",
       "                                               option_b  \\\n",
       " 0                     simplificarea unei idei complexe   \n",
       " 1                                  se mișcă mai liber.   \n",
       " 2    Apa călduță a dizolvat cea mai multă cantitate...   \n",
       " 3          Fluturii sunt mai buni la zbor decât molii.   \n",
       " 4      Trebuie versată într-un container de reciclare.   \n",
       " ..                                                 ...   \n",
       " 292  Materia veche a fost distrusă când apa s-a tra...   \n",
       " 293  folosind mănuși rezistente la căldură dacă vas...   \n",
       " 294  Temperatura lichidului a crescut sau lichidul ...   \n",
       " 295  Vaporul de apă trebuie să piardă energia termică.   \n",
       " 296                                  plin și nouă lună   \n",
       " \n",
       "                                            instruction  \n",
       " 0    Modelele științifice sunt foarte comune. Pentr...  \n",
       " 1    Pe măsură ce apa începe să înghețe, moleculele...  \n",
       " 2    Sue a măsurat cât de mult zahăr se poate dizol...  \n",
       " 3    Care comparație dintre fluturi și molii este o...  \n",
       " 4    În timpul orei de științe se realizează o solu...  \n",
       " ..                                                 ...  \n",
       " 292  Un grup de studenți a observat formarea de bul...  \n",
       " 293  Care acțiune nu reprezintă o procedură de sigu...  \n",
       " 294  În timpul unei investigații, căldura s-a trans...  \n",
       " 295  Ce trebuie să se întâmple înainte ca norii să ...  \n",
       " 296  Înălțimile mareelor oceanice ale Pământului su...  \n",
       " \n",
       " [296 rows x 7 columns],\n",
       " 'test':                                                option_d  \\\n",
       " 0                       Atomul devine încărcat negativ.   \n",
       " 1                                       topirea crustei   \n",
       " 2                                                  bec.   \n",
       " 3                 Roca este formată din sediment depus.   \n",
       " 4     previziunea schimbărilor viitoare în producția...   \n",
       " ...                                                 ...   \n",
       " 1162  Descendenții balenelor cu fanere moderne vor a...   \n",
       " 1163           fusează hidrogen pentru a produce heliu.   \n",
       " 1164           mingea nu a fost aruncată direct în sus.   \n",
       " 1165  o placă fierbinte care încălzește o sticlă de ...   \n",
       " 1166  sticlă de plastic verde pentru băuturi răcorit...   \n",
       " \n",
       "                                         id  \\\n",
       " 0       ARC-Challenge/test/Mercury_7097493   \n",
       " 1       ARC-Challenge/test/Mercury_7093958   \n",
       " 2     ARC-Challenge/test/Mercury_SC_401168   \n",
       " 3        ARC-Challenge/test/OHAT_2007_8_24   \n",
       " 4       ARC-Challenge/test/Mercury_7222460   \n",
       " ...                                    ...   \n",
       " 1162      ARC-Challenge/test/MCAS_2006_9_4   \n",
       " 1163    ARC-Challenge/test/Mercury_7009993   \n",
       " 1164      ARC-Challenge/test/LEAP_2000_8_4   \n",
       " 1165     ARC-Challenge/test/MDSA_2008_8_20   \n",
       " 1166  ARC-Challenge/test/Mercury_SC_400839   \n",
       " \n",
       "                                                option_c answer  \\\n",
       " 0                        Atomul crește în masă atomică.      D   \n",
       " 1                                  încălzirea nucleului      A   \n",
       " 2                                         întrerupător.      C   \n",
       " 3       Roca se solidifică încet în adâncuri subterane.      A   \n",
       " 4     preocupări legate de sănătate privind efectele...      A   \n",
       " ...                                                 ...    ...   \n",
       " 1162          Balenelor cu fanere li se dezvoltă dinți.      A   \n",
       " 1163            are o densitate și gravitație mai mari.      A   \n",
       " 1164          mingea nu a fost aruncată destul de tare.      B   \n",
       " 1165                       soarele care încălzește luna      C   \n",
       " 1166       recipient de hârtie pentru suc de portocale.      B   \n",
       " \n",
       "                                               option_a  \\\n",
       " 0                             Atomul pierde un proton.   \n",
       " 1                                    eroziunea rocilor   \n",
       " 2                                                 fir.   \n",
       " 3               Roca se răcește rapid din rocă topită.   \n",
       " 4          investigarea cauzelor fenomenelor observate   \n",
       " ...                                                ...   \n",
       " 1162  Balenelor primitive li se cresc dinți ca adulți.   \n",
       " 1163                arde la o temperatură mai scăzută.   \n",
       " 1164                fricțiunea aerului a oprit mingea.   \n",
       " 1165                    cafea care încălzește o ceașcă   \n",
       " 1166                recipient de plastic pentru iaurt.   \n",
       " \n",
       "                                                option_b  \\\n",
       " 0                 Atomul atrage electroni suplimentari.   \n",
       " 1                                       evaporarea apei   \n",
       " 2                                              baterie.   \n",
       " 3       Roca este recristalizată prin presiune extremă.   \n",
       " 4     găsirea de opțiuni pentru înlocuirea lungimilo...   \n",
       " ...                                                 ...   \n",
       " 1162  Balenelor cu dinți le-au descins din balenele ...   \n",
       " 1163        este întotdeauna mai mare decât alte soare.   \n",
       " 1164             gravitația a schimbat direcția mingii.   \n",
       " 1165                aerul cald care încălzește o cameră   \n",
       " 1166           borcan de sticlă etichetat cu claritate.   \n",
       " \n",
       "                                             instruction  \n",
       " 0     Ce se întâmplă atunci când un atom câștigă un ...  \n",
       " 1     Care este unul dintre efectele pe care hidrosf...  \n",
       " 2     Curentul care circulă printr-un circuit este o...  \n",
       " 3     Care este procesul major de formare a rocilor ...  \n",
       " 4     În 1879, Marie Cornu a descoperit că lumina de...  \n",
       " ...                                                 ...  \n",
       " 1162  Există două tipuri de balene moderne: balene c...  \n",
       " 1163  O stea gigant roșie se diferențiază de stelele...  \n",
       " 1164  Jerry a aruncat o minge in aer. Aceasta a urma...  \n",
       " 1165  Se transferă căldură de la un obiect la alt ob...  \n",
       " 1166  Cel mai bun recipient pentru stocarea unui lic...  \n",
       " \n",
       " [1164 rows x 7 columns]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('OpenLLM-Ro/ro_arc_challenge', trust_remote_code=True)\n",
    "\n",
    "def preprocess_splits(dataset):\n",
    "    train_df = dataset['train'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "    test_df = dataset['test'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "    validation_df = dataset['validation'].to_pandas().sample(frac = 1, random_state = 1).reset_index(drop=True)\n",
    "\n",
    "    train_df = train_df[train_df['answer'] != 'E']\n",
    "    train_df = train_df[train_df['option_e'].isnull()]\n",
    "    test_df = test_df[test_df['answer'] != 'E']\n",
    "    test_df = test_df[test_df['option_e'].isnull()]\n",
    "    validation_df = validation_df[validation_df['answer'] != 'E']\n",
    "    validation_df = validation_df[validation_df['option_e'].isnull()]\n",
    "    for df in (train_df, validation_df, test_df):\n",
    "        df.drop(columns=[\"option_e\"], inplace=True)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": train_df,\n",
    "        \"validation\": validation_df,\n",
    "        \"test\": test_df\n",
    "    }\n",
    "    return splits\n",
    "\n",
    "splits = preprocess_splits(dataset)\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd9101b-6e71-41e3-a03b-dc40de1753a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from enum import Enum\n",
    "import os\n",
    "\n",
    "MODEL_NORMALIZATION = {\n",
    "    \"faur-ai/LLMic\": True\n",
    "}\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "class EmbeddingExtractor:\n",
    "    def __init__(self, model, tokenizer, model_name, device=None, pooling=\"classical-avg\"):\n",
    "        self.model = model.to(device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")).eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.pooling = pooling\n",
    "        self.model_name = model_name\n",
    "        self.model_needs_normalization = MODEL_NORMALIZATION.get(self.model_name, False)\n",
    "        self.max_length = getattr(self.model.config, \"max_position_embeddings\")\n",
    "        \n",
    "    def _build_prompt(self, text, strategy):\n",
    "        if strategy == \"echo\":\n",
    "            prompt = f\"Rescrie întrebarea cu patru variante de răspuns:\\n\\n{text}\\n\\nÎntrebarea rescrisă:\\n\\n{text}\"\n",
    "        elif strategy == \"summary\":\n",
    "            prompt = f\"Scrie întrebarea cu patru variante de răspuns:\\n\\n{text}\\n\\nCare este un singur cuvânt cheie relevant pentru întrebare?\"\n",
    "        else:\n",
    "            prompt = f\"Scrie întrebarea cu patru variante de răspuns:\\n\\n{text}\"\n",
    "\n",
    "        if self.model_needs_normalization:\n",
    "            return remove_diacritics(prompt).lower()\n",
    "        return prompt\n",
    "\n",
    "    def _apply_pooling(self, hidden_states, inputs, text, strategy, pooling_method):\n",
    "        input_ids = inputs[\"input_ids\"][0]\n",
    "        full_text = self.tokenizer.decode(input_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if self.model_needs_normalization:\n",
    "            full_text = remove_diacritics(full_text).lower()\n",
    "\n",
    "        # print(f\"\\n[DEBUG] Full Decoded Prompt:\\n{full_text}\\n\")\n",
    "\n",
    "        if strategy == \"echo\":\n",
    "            instruction = \"intrebarea rescrisa:\" if self.model_needs_normalization else \"Întrebarea rescrisă:\"\n",
    "            idx = full_text.find(instruction)\n",
    "            if idx == -1:\n",
    "                raise ValueError(f\"Failed to find echo instruction '{instruction}' in the prompt text.\")\n",
    "            selected_text = full_text[idx + len(instruction):].strip()\n",
    "            # print(f\"[DEBUG] Selected (echo second occurrence):\\n{selected_text}\\n\")\n",
    "            second_tokens = self.tokenizer(selected_text, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = second_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :]\n",
    "\n",
    "        elif strategy == \"summary\":\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=10,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "            generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            completion = generated_text[len(full_text):].strip()\n",
    "            if not completion:\n",
    "                print(\"[WARNING] Model did not generate anything after summary instruction.\")\n",
    "            # print(f\"[DEBUG] Generated Text (summary):\\n{completion}\\n\")\n",
    "            first_word = completion.split()[0] if completion else \"\"\n",
    "            first_word = next((w for w in completion.split() if w.strip(\"-.,!?…\").isalnum()), \"\")\n",
    "            # print(f\"[DEBUG] Generated Text First Word (summary):\\n{first_word}\\n\")\n",
    "           \n",
    "            summary_tokens = self.tokenizer(first_word, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = summary_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :] if length > 0 else hidden_states[:, -1:, :]\n",
    "\n",
    "        else:\n",
    "            instruction = \"scrie intrebarea cu patru variante de raspuns:\" if self.model_needs_normalization else \"Scrie întrebarea cu patru variante de răspuns:\"\n",
    "            idx = full_text.find(instruction)\n",
    "            if idx == -1:\n",
    "                raise ValueError(f\"Failed to find classical instruction '{instruction}' in the prompt text.\")\n",
    "            after_instruction = full_text[idx + len(instruction):].strip()\n",
    "            # print(f\"[DEBUG] Selected (classical real input):\\n{after_instruction}\\n\")\n",
    "            text_tokens = self.tokenizer(after_instruction, return_tensors=\"pt\", truncation=True).to(self.device)\n",
    "            length = text_tokens[\"input_ids\"].shape[1] - 2\n",
    "            selected = hidden_states[:, -length:, :]\n",
    "\n",
    "        if pooling_method == \"avg\":\n",
    "            return selected.mean(dim=1).squeeze()\n",
    "        elif pooling_method == \"last\":\n",
    "            return selected[:, -1, :].squeeze()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling method: {pooling_method}\")\n",
    "\n",
    "\n",
    "    def extract_single(self, text):\n",
    "        strategy, pooling_method = self.pooling.split(\"-\")\n",
    "        prompt = self._build_prompt(text, strategy)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True,  max_length=self.max_length).to(self.device)\n",
    "        \n",
    "        # print(\"[DEBUG] Decoded back:\", self.tokenizer.decode(inputs[\"input_ids\"][0])) # see how model internally tokenizes data (lowercase etc) llmic fara diactritice si lowercase\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states[-1]\n",
    "        text = remove_diacritics(text).lower() if self.model_needs_normalization else text\n",
    "        return self._apply_pooling(hidden_states, inputs, text, strategy, pooling_method)\n",
    "\n",
    "    def extract_batch(self, texts, save_path=None, save_format=\"pt\"):\n",
    "        embeddings = []\n",
    "\n",
    "        for text in tqdm(texts, desc=f\"Extracting ({self.pooling})\"):\n",
    "            emb = self.extract_single(text)\n",
    "            embeddings.append(emb.cpu())\n",
    "\n",
    "        stacked = torch.stack(embeddings)\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            if save_format == \"pt\":\n",
    "                torch.save(stacked, save_path)\n",
    "            elif save_format == \"npy\":\n",
    "                import numpy as np\n",
    "                np.save(save_path, stacked.numpy())\n",
    "\n",
    "        return stacked\n",
    "\n",
    "    def extract_in_chunks(extractor, texts, save_path, save_format=\"pt\", batch_size=64):\n",
    "        all_embeddings = []\n",
    "    \n",
    "        total = len(texts)\n",
    "        with tqdm(total=total, desc=f\"Extracting ({extractor.pooling})\") as pbar:\n",
    "            for i in range(0, total, batch_size):\n",
    "                batch = texts[i:i + batch_size]\n",
    "                batch_emb = extractor.extract_batch(batch)  \n",
    "                all_embeddings.append(batch_emb)\n",
    "                pbar.update(len(batch))\n",
    "    \n",
    "        stacked = torch.cat(all_embeddings)\n",
    "        return stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c56af3-34fe-4e30-bd88-875273418851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingExtractionRunner:\n",
    "    def __init__(self, model, tokenizer, model_name, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_name = model_name\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def run(self, splits, save_root):\n",
    "        strategies = [\"classical-avg\", \"classical-last\", \"echo-avg\", \"summary-avg\"]\n",
    "\n",
    "        for strategy in strategies:\n",
    "            extractor = EmbeddingExtractor(\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                model_name=self.model_name,\n",
    "                device=self.device,\n",
    "                pooling=strategy\n",
    "            )\n",
    "\n",
    "            for split_name, split_data in splits.items():\n",
    "                texts = [ \n",
    "                    f\"{row['instruction']}\\nA. {row['option_a']}\\nB. {row['option_b']}\\nC. {row['option_c']}\\nD. {row['option_d']}\"\n",
    "                    for _, row in split_data.iterrows()\n",
    "                ]\n",
    "\n",
    "                base_dir = os.path.join(\n",
    "                    save_root,\n",
    "                    self.model_name,\n",
    "                    strategy,\n",
    "                    split_name\n",
    "                )\n",
    "                os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "                npy_path = os.path.join(base_dir, \"embeddings.npy\")\n",
    "\n",
    "                embeddings = extractor.extract_batch(texts)\n",
    "                np.save(npy_path, embeddings.cpu().numpy())\n",
    "\n",
    "                del embeddings\n",
    "                torch.cuda.empty_cache()\n",
    "                import gc\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d43a7-d3ab-4f93-be12-5e5d89f63ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f53acd-2c5c-4b02-9334-3ea1560b0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "MODELS = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"OpenLLM-Ro/RoLlama3.1-8b-Instruct\",\n",
    "    \"ai-forever/mGPT-1.3B-romanian\",\n",
    "    \"faur-ai/LLMic\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2244db03",
   "metadata": {},
   "source": [
    "Debug to make sure everything works for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b26fe-f71a-41c6-8c66-fb2f5144dfbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strategies = [\"classical-avg\", \"classical-last\", \"echo-avg\", \"summary-avg\"]\n",
    "for model_name in MODELS:\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    for strategy in strategies:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        extractor = EmbeddingExtractor(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_name=model_name,\n",
    "        device=\"cuda\",\n",
    "        pooling=strategy\n",
    "        )\n",
    "        \n",
    "        row = splits[\"train\"].iloc[0]\n",
    "        text = f\"{row['instruction']}\\nA. {row['option_a']}\\nB. {row['option_b']}\\nC. {row['option_c']}\\nD. {row['option_d']}\"\n",
    "        \n",
    "        embedding = extractor.extract_single(text)\n",
    "        \n",
    "        print(f\"Embedding shape: {embedding.shape}\")\n",
    "        print(f\"Embedding preview: {embedding[:10]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cedce1f",
   "metadata": {},
   "source": [
    "Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d493332-dca4-4bd0-b36a-30466f715e59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f7dec4b564d6484ae0e2cfd6d8130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting (classical-avg): 100% 1108/1108 [01:04<00:00, 17.20it/s]\n",
      "Extracting (classical-avg): 100% 296/296 [00:17<00:00, 16.63it/s]\n",
      "Extracting (classical-avg): 100% 1164/1164 [01:09<00:00, 16.80it/s]\n",
      "Extracting (classical-last): 100% 1108/1108 [01:04<00:00, 17.18it/s]\n",
      "Extracting (classical-last): 100% 296/296 [00:17<00:00, 16.64it/s]\n",
      "Extracting (classical-last): 100% 1164/1164 [01:09<00:00, 16.81it/s]\n",
      "Extracting (echo-avg): 100% 1108/1108 [01:43<00:00, 10.66it/s]\n",
      "Extracting (echo-avg): 100% 296/296 [00:28<00:00, 10.34it/s]\n",
      "Extracting (echo-avg): 100% 1164/1164 [01:51<00:00, 10.40it/s]\n",
      "Extracting (summary-avg): 100% 1108/1108 [04:59<00:00,  3.70it/s]\n",
      "Extracting (summary-avg): 100% 296/296 [01:21<00:00,  3.64it/s]\n",
      "Extracting (summary-avg):  90% 1051/1164 [04:46<00:32,  3.48it/s]"
     ]
    }
   ],
   "source": [
    "for model_name in MODELS:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    runner = EmbeddingExtractionRunner(model, tokenizer, model_name)\n",
    "    runner.run(splits, save_root=\"roarc_embeddings\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
